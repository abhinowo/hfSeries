{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Food-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset('food101', split='train[:5000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512>,\n",
       " 'label': 53}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at an example\n",
    "food['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food['train'].features['label'].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime_rib'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "The next step is to load a ViT image processor to process the image into a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pytorch\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, ToTensor, Normalize\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "size = (\n",
    "    processor.size['shortest_edge']\n",
    "    if 'shortest_edge' in processor.size\n",
    "    else processor.size['height'], processor.size['width']\n",
    ")\n",
    "\n",
    "transform = Compose([RandomResizedCrop(size=size), ToTensor(), normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples['pixel_values'] = [transform(img.convert('RGB')) for img in examples['image']]\n",
    "    del examples['image']\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transforms(examples):\n",
    "#     examples[\"pixel_values\"] = [\n",
    "#         transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n",
    "#     ]\n",
    "\n",
    "#     return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "To avoid overfitting and to make the model more robust, add some data augmentation to the training part of the dataset. Here we use Keras preprocessing layers to define the transformations for the training data (includes data augmentation), and transformations for the validation data (only center cropping, resizing and normalizing). You can use tf.imageor any other library you prefer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "size = (processor.size['height'], processor.size['width'])\n",
    "\n",
    "train_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2, fill_mode=\"constant\"\n",
    "        ),\n",
    "    ],\n",
    "    name = \"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name = 'val_data_augmentation',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    images = [ train_data_augmentation(convert_to_tf_tensor(image.convert('RGB'))) for image in example_batch['image'] ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    images = [ val_data_augmentation(convert_to_tf_tensor(image.convert('RGB'))) for image in example_batch['image'] ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"].set_transform(preprocess_train)\n",
    "food[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "\n",
    "train finetuning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    seed=42,\n",
    "    log_level='error',\n",
    "    # evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n",
    "\n",
    "fine-tuning a model with keras\n",
    "To fine-tune a model in TensorFlow, follow these steps:\n",
    "\n",
    "1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n",
    "2. Instantiate a pre-trained model.\n",
    "3. Convert a ðŸ¤— Dataset to a tf.data.Dataset.\n",
    "4. Compile your model.\n",
    "5. Add callbacks and use the fit() method to run the training.\n",
    "6. Upload your model to ðŸ¤— Hub to share with the community.\n",
    "\n",
    "Start by defining the hyperparameters, optimizer and learning rate schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_train_steps = len(food['train']) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr = learning_rate,\n",
    "    num_train_steps = num_train_steps,\n",
    "    weight_decay_rate = weight_decay_rate,\n",
    "    num_warmup_steps = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on terminal to see thransformers \n",
    "# python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load ViT with TFAutoModelForImageClassification along with the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = food['train'].to_tf_dataset(\n",
    "    columns='pixel_values', \n",
    "             label_cols='label',\n",
    "              shuffle=True,\n",
    "               batch_size=batch_size,\n",
    "                collate_fn=data_collator, \n",
    ")\n",
    "\n",
    "tf_eval_dataset = food['test'].to_tf_dataset(\n",
    "    columns='pixel_values', \n",
    "             label_cols='label',\n",
    "              shuffle=True,\n",
    "               batch_size=batch_size,\n",
    "                collate_fn=data_collator, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/damiacc2/food_classifier into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"food_classifier\",\n",
    "    tokenizer=processor,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback, push_to_hub_callback]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 834s 3s/step - loss: 0.4267 - accuracy: 0.9090 - val_loss: 0.3843 - val_accuracy: 0.9090\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 799s 3s/step - loss: 0.3467 - accuracy: 0.9170 - val_loss: 0.3304 - val_accuracy: 0.9170\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 798s 3s/step - loss: 0.2926 - accuracy: 0.9130 - val_loss: 0.3178 - val_accuracy: 0.9130\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 746s 3s/step - loss: 0.2469 - accuracy: 0.9170 - val_loss: 0.3025 - val_accuracy: 0.9170\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 743s 3s/step - loss: 0.2269 - accuracy: 0.9260 - val_loss: 0.2786 - val_accuracy: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload file tf_model.h5: 336MB [00:27, 20.2MB/s]                            To https://huggingface.co/damiacc2/food_classifier\n",
      "   ecc7655..f787658  main -> main\n",
      "\n",
      "Upload file tf_model.h5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 328M/328M [00:28<00:00, 12.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f93b0539a50>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "Load an image youâ€™d like to run inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('food101', split='validation[:10]')\n",
    "image = ds['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9651563167572021, 'label': 'beignets'},\n",
       " {'score': 0.0036974535323679447, 'label': 'chicken_wings'},\n",
       " {'score': 0.0035024266690015793, 'label': 'prime_rib'},\n",
       " {'score': 0.0022872083354741335, 'label': 'pork_chop'},\n",
       " {'score': 0.002025170950219035, 'label': 'hamburger'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('image-classification', model='food_classifier', tokenizer=processor)\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained('food_classifier')\n",
    "inputs = image_processor(image, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained('food_classifier')\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_class_id = int(tf.argmax(logits[0]))\n",
    "# predicted_label = image_processor.labels[predicted_class_id]\n",
    "# print(\"Predicted label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beignets'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
